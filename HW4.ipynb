{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set(string.ascii_lowercase + \" \")\n",
    "langs = {\"English\":\"e\", \"Japanese\":\"j\", \"Spanish\":\"s\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns disctionary of chars and their counts \n",
    "def read_file(file_name):    \n",
    "    data_file = open(os.getcwd()+\"/languageID/\"+file_name, \"r\")\n",
    "    text = data_file.read()\n",
    "    data_file.close()\n",
    "    \n",
    "    counts = {}\n",
    "    for char in s:\n",
    "        counts[char] = text.count(char)\n",
    "    \n",
    "    return dict(sorted(counts.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return dictionary of dictionaries\n",
    "# outer key: file_name\n",
    "# inner key: char\n",
    "# value: count\n",
    "def get_training_data():\n",
    "    counts_dict_dict = {}\n",
    "    for file_name in sorted(os.listdir(os.getcwd()+\"/languageID/\")):\n",
    "        if re.search(\"^(e|j|s)[0-9](?!\\d)\", file_name) is not None:\n",
    "            counts_dict_dict[file_name] = read_file(file_name)\n",
    "            \n",
    "    return counts_dict_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the prior probability of a language\n",
    "# default additive smoothing parameter 1/2\n",
    "def get_prior_prob(training_data, lang, a=0.5):\n",
    "    count = 0\n",
    "    for file_name in training_data:\n",
    "        if re.search(\"^\"+lang, file_name) is not None:\n",
    "            count += 1\n",
    "            \n",
    "    return (count + a)/(len(training_data) + a*len(langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns dictionary of chars and their corresponding probabilities in training data given a language\n",
    "# default smoothing parameter 1/2\n",
    "def get_class_cond_probs(training_data, lang=\"all\", a=0.5):\n",
    "    \n",
    "    # initialize return variable\n",
    "    class_cond_probs = {}\n",
    "    for char in s:\n",
    "        class_cond_probs[char] = 0\n",
    "    \n",
    "    # getting the counts conditioned on language\n",
    "    if not lang == \"all\": \n",
    "        for file_name in training_data:\n",
    "            if re.search(\"^\"+lang, file_name) is not None:\n",
    "                for char in s:\n",
    "                    class_cond_probs[char] += training_data[file_name][char]\n",
    "    # getting the counts not conditioned on language\n",
    "    else:\n",
    "        for file_name in training_data:\n",
    "            for char in s:\n",
    "                class_cond_probs[char] += training_data[file_name][char]\n",
    "    \n",
    "    # getting the probs with smoothing    \n",
    "    tot_chars = sum(class_cond_probs.values())\n",
    "    \n",
    "    for char in class_cond_probs:\n",
    "        class_cond_probs[char] = (class_cond_probs[char] + a)/(tot_chars + a*len(s))\n",
    "    \n",
    "    return dict(sorted(class_cond_probs.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns dictionary of chars and their corresponding probabilities in the test_data\n",
    "def get_probs_x(test_data, a=0.5):\n",
    "    probs = {}\n",
    "    for char in s:\n",
    "        probs[char] = test_data[char]\n",
    "        \n",
    "    tot_chars = sum(probs.values())\n",
    "        \n",
    "    for char in probs:\n",
    "        probs[char] /= tot_chars \n",
    "        \n",
    "    return dict(sorted(probs.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the log probability of count-vextor x given conditional probability vector theta\n",
    "def p_of_x_given_y(x, theta):\n",
    "    prob = 0\n",
    "    for char in s:\n",
    "        prob += x[char] * np.log(theta[char])\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 0.3333333333333333\n",
      "Japanese: 0.3333333333333333\n",
      "Spanish: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data()\n",
    "for lang in langs:\n",
    "    print(lang + \": \" + str(get_prior_prob(training_data, langs[lang])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Class Conditional Probabilities:\n",
      "(' ', 0.1792499586981662)\n",
      "('e', 0.1053692383941847)\n",
      "('t', 0.08012555757475633)\n",
      "('s', 0.06618205848339666)\n",
      "('o', 0.06446390219725756)\n",
      "('a', 0.0601685114819098)\n",
      "('n', 0.057921691723112505)\n",
      "('i', 0.055410540227986124)\n",
      "('r', 0.053824549810011564)\n",
      "('h', 0.047216256401784236)\n",
      "('l', 0.028977366595076822)\n",
      "('u', 0.026664463902197257)\n",
      "('d', 0.021972575582355856)\n",
      "('c', 0.021509995043779945)\n",
      "('m', 0.020518751032545846)\n",
      "('f', 0.018932760614571286)\n",
      "('g', 0.017478936064761277)\n",
      "('p', 0.01675202378985627)\n",
      "('w', 0.015496448042293078)\n",
      "('y', 0.013844374690236246)\n",
      "('b', 0.011134974392863043)\n",
      "('v', 0.009284652238559392)\n",
      "('k', 0.0037336857756484387)\n",
      "('j', 0.001420783082768875)\n",
      "('x', 0.001156451346439782)\n",
      "('z', 0.0006277878737815959)\n",
      "('q', 0.0005617049396993227)\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data()\n",
    "class_cond_probs = get_class_cond_probs(training_data, lang=\"e\")\n",
    "\n",
    "print(\"English Class Conditional Probabilities:\")\n",
    "for item in class_cond_probs.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese Class Conditional Probabilities:\n",
      "('a', 0.1317656102589189)\n",
      "(' ', 0.12344945665466997)\n",
      "('i', 0.09703343932352633)\n",
      "('o', 0.09116321324993885)\n",
      "('u', 0.07061742199238269)\n",
      "('e', 0.06020475907613823)\n",
      "('k', 0.05740941332681086)\n",
      "('t', 0.056990111464411755)\n",
      "('n', 0.05671057688947902)\n",
      "('r', 0.04280373178657535)\n",
      "('s', 0.0421747789929767)\n",
      "('m', 0.03979873510604843)\n",
      "('h', 0.03176211607673224)\n",
      "('w', 0.01974212935462455)\n",
      "('d', 0.01722631818022992)\n",
      "('y', 0.01415143785596981)\n",
      "('g', 0.014011670568503443)\n",
      "('b', 0.010866906600510151)\n",
      "('z', 0.00772214263251686)\n",
      "('c', 0.005485866033054963)\n",
      "('f', 0.003878542227191726)\n",
      "('j', 0.0023411020650616725)\n",
      "('l', 0.001432614696530277)\n",
      "('p', 0.0008735455466648031)\n",
      "('v', 0.0002445927530661449)\n",
      "('q', 0.00010482546559977637)\n",
      "('x', 3.4941821866592126e-05)\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data()\n",
    "class_cond_probs = get_class_cond_probs(training_data, lang=\"j\")\n",
    "\n",
    "print(\"Japanese Class Conditional Probabilities:\")\n",
    "for item in class_cond_probs.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish Class Conditional Probabilities:\n",
      "(' ', 0.16826493170115014)\n",
      "('e', 0.1138108599796491)\n",
      "('a', 0.10456045141993771)\n",
      "('o', 0.07249236841293824)\n",
      "('s', 0.06577040485954797)\n",
      "('r', 0.05929511886774999)\n",
      "('n', 0.054176559464709693)\n",
      "('l', 0.052943171656748174)\n",
      "('i', 0.049859702136844375)\n",
      "('d', 0.039745922111559924)\n",
      "('c', 0.03752582405722919)\n",
      "('t', 0.03561407295488884)\n",
      "('u', 0.03370232185254849)\n",
      "('m', 0.02580863988159477)\n",
      "('p', 0.02426690512164287)\n",
      "('f', 0.00860287996053159)\n",
      "('b', 0.008232863618143134)\n",
      "('y', 0.007862847275754679)\n",
      "('q', 0.007677839104560451)\n",
      "('g', 0.0071844839813758445)\n",
      "('j', 0.006629459467793161)\n",
      "('v', 0.00588942678301625)\n",
      "('h', 0.0045327001942585795)\n",
      "('z', 0.0026826184823163022)\n",
      "('x', 0.0024976103111220747)\n",
      "('k', 0.0002775122567913416)\n",
      "('w', 9.250408559711388e-05)\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data()\n",
    "class_cond_probs = get_class_cond_probs(training_data, lang=\"s\")\n",
    "\n",
    "print(\"Spanish Class Conditional Probabilities:\")\n",
    "for item in class_cond_probs.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' ', 498)\n",
      "('e', 311)\n",
      "('t', 225)\n",
      "('s', 186)\n",
      "('o', 182)\n",
      "('a', 164)\n",
      "('r', 141)\n",
      "('h', 140)\n",
      "('i', 140)\n",
      "('n', 139)\n",
      "('l', 85)\n",
      "('u', 65)\n",
      "('m', 64)\n",
      "('d', 57)\n",
      "('f', 55)\n",
      "('p', 53)\n",
      "('c', 53)\n",
      "('g', 51)\n",
      "('w', 47)\n",
      "('y', 38)\n",
      "('b', 32)\n",
      "('v', 31)\n",
      "('k', 6)\n",
      "('x', 4)\n",
      "('q', 3)\n",
      "('j', 3)\n",
      "('z', 2)\n"
     ]
    }
   ],
   "source": [
    "x = read_file(\"e10.txt\")\n",
    "for item in x.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood - English:  -7841.865447060635\n",
      "Log Likelihood - Japanese: -8771.43307907503\n",
      "Log Likelihood - Spanish:  -8467.282044010557\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data()\n",
    "theta_e = get_class_cond_probs(training_data, lang=\"e\")\n",
    "theta_j = get_class_cond_probs(training_data, lang=\"j\")\n",
    "theta_s = get_class_cond_probs(training_data, lang=\"s\")\n",
    "x = read_file(\"e10.txt\")\n",
    "\n",
    "print(\"Log Likelihood - English:  \" + str(p_of_x_given_y(x, theta_e)))\n",
    "print(\"Log Likelihood - Japanese: \" + str(p_of_x_given_y(x, theta_j)))\n",
    "print(\"Log Likelihood - Spanish:  \" + str(p_of_x_given_y(x, theta_s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns log(p(y|x) * p(x))\n",
    "def get_predicted_prob(training_data, test_data, lang):\n",
    "    theta_lang = get_class_cond_probs(training_data, lang=lang)\n",
    "    theta_all = get_class_cond_probs(training_data, lang=\"all\")\n",
    "    x = test_data\n",
    "        \n",
    "    return p_of_x_given_y(x, theta_lang) + np.log(get_prior_prob(training_data, lang))\n",
    "\n",
    "# Below returns atual probabilites, uneccessary for homework per Piaza\n",
    "\n",
    "#     return (get_prior_prob(training_data, lang) \n",
    "#             * math.e**(p_of_x_given_y(test_data, theta_lang) \n",
    "#             - p_of_x_given_y(test_data, theta_all)))\n",
    "\n",
    "#     return math.e**(p_of_x_given_y(x, theta_lang)\n",
    "#                     + np.log(get_prior_prob(training_data, lang))\n",
    "#                     - p_of_x_given_y(x, theta_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: -7842.964059349303\n",
      "Japanese: -8772.531691363698\n",
      "Spanish: -8468.380656299225\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data()\n",
    "test_data = read_file(\"e10.txt\")\n",
    "\n",
    "for lang in langs:\n",
    "    print(lang + \": \" + str(get_predicted_prob(training_data, test_data, langs[lang])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(training_data, test_data):\n",
    "    \n",
    "    probs = {}\n",
    "    for lang in langs:\n",
    "        theta = get_class_cond_probs(training_data, langs[lang])\n",
    "        probs[lang] = get_predicted_prob(training_data, test_data, langs[lang])\n",
    "    \n",
    "    return max(probs, key=probs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data()\n",
    "test_data = read_file(\"e10.txt\")\n",
    "\n",
    "print(make_prediction(training_data, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Predicted\n",
      "-----------------\n",
      "English: English\n",
      "English: English\n",
      "English: English\n",
      "English: English\n",
      "English: English\n",
      "English: English\n",
      "English: English\n",
      "English: English\n",
      "English: English\n",
      "English: English\n",
      "Japanese: Japanese\n",
      "Japanese: Japanese\n",
      "Japanese: Japanese\n",
      "Japanese: Japanese\n",
      "Japanese: Japanese\n",
      "Japanese: Japanese\n",
      "Japanese: Japanese\n",
      "Japanese: Japanese\n",
      "Japanese: Japanese\n",
      "Japanese: Japanese\n",
      "Spanish: Spanish\n",
      "Spanish: Spanish\n",
      "Spanish: Spanish\n",
      "Spanish: Spanish\n",
      "Spanish: Spanish\n",
      "Spanish: Spanish\n",
      "Spanish: Spanish\n",
      "Spanish: Spanish\n",
      "Spanish: Spanish\n",
      "Spanish: Spanish\n",
      "\n",
      "Files Tested: \n",
      "e10.txt\n",
      "e11.txt\n",
      "e12.txt\n",
      "e13.txt\n",
      "e14.txt\n",
      "e15.txt\n",
      "e16.txt\n",
      "e17.txt\n",
      "e18.txt\n",
      "e19.txt\n",
      "j10.txt\n",
      "j11.txt\n",
      "j12.txt\n",
      "j13.txt\n",
      "j14.txt\n",
      "j15.txt\n",
      "j16.txt\n",
      "j17.txt\n",
      "j18.txt\n",
      "j19.txt\n",
      "s10.txt\n",
      "s11.txt\n",
      "s12.txt\n",
      "s13.txt\n",
      "s14.txt\n",
      "s15.txt\n",
      "s16.txt\n",
      "s17.txt\n",
      "s18.txt\n",
      "s19.txt\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data()\n",
    "\n",
    "test_file_names = []\n",
    "print(\"Actual: Predicted\\n-----------------\")\n",
    "for file_name in sorted(os.listdir(os.getcwd()+\"/languageID/\")):\n",
    "    if re.search(\"^(e|j|s)[0-9]{2}(?!\\d)\", file_name) is not None:\n",
    "        test_data = read_file(file_name)\n",
    "        test_file_names.append(file_name)\n",
    "    \n",
    "        print(list(langs.keys()) [list(langs.values()).index(file_name[0])] + \": \" +\n",
    "             make_prediction(training_data, test_data))\n",
    "        \n",
    "print(\"\\nFiles Tested: \")\n",
    "for name in test_file_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Without Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cuales son los problemas\n",
      "\n",
      "la depilacion de las cejas siempre debe seguir su linea natural hasta hacerlo impecable depilar demasiado las cejas es privar al rostro de personalidad mientras que no depilarlas supone lucir una mirada poco cuidada\n",
      "\n",
      "son varias la razones por las que la piel de este area es mas susceptible a las arrugas la sequedad y la flacidez la piel del contorno de ojos tiene un espesor insignificante  mm mientras que la del resto del rostro posee mm y la del cuello mm presenta escasas glandulas sebaceas y las sudoriparas aunque numerosas resultan extremadamente pequenas\n",
      "\n",
      "entre ceja y ceja\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "publicidad\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name = \"s11.txt\"\n",
    "\n",
    "data_file = open(os.getcwd()+\"/languageID/\"+file_name, \"r\")\n",
    "text = data_file.read()\n",
    "data_file.close()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data()\n",
    "\n",
    "test_data = {}\n",
    "for char in s:\n",
    "    test_data[char] = text.count(char)\n",
    "    \n",
    "print(make_prediction(training_data, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - With Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lres eecraoosmpicu a\n",
      " os  taa aec le\n",
      "p esdd  o ar   amylro amsaudisuleataayujesapsrs nscoleli lsri\n",
      "e  \n",
      "anu  \n",
      "dpl eti\n",
      "ncy sed\n",
      "eaene e dpmzelimis lpqystgraell a\n",
      "as de llp\n",
      "zaupnlae sseildevonu\n",
      "uietd eaaeepq\n",
      "nseeor ca\n",
      "rrmauu r nrsso seplisnsesundfpsesjsi eaeilda jrenneoesciessstst iarcssorsdirriastiuaoaasanan\n",
      "smoeluax\n",
      "oc  seie     rleiqq ilg\n",
      "alnepmlam\n",
      "adln ncassmaosc d\n",
      "eaasponl \n",
      "aoaele  nae tenvir faed a cpeeba elaa s  \n",
      "je eheltrcutlom dblias ss  s\n",
      "atie  rn mnraejaoddr aeos \n",
      "ae el dc  aq  remneol irtc\n",
      " oroa eor\n",
      "c a olsrs  a pdeaoeuemubhat  duiptu\n",
      " ranurcp  \n",
      "padgc aem dodl\n",
      "slstetml\n",
      "ea e \n",
      "aaluabbl rnrnomlippriddrblue   oeanuqi uilga euca d\n"
     ]
    }
   ],
   "source": [
    "file_name = \"s11.txt\"\n",
    "data_file = open(os.getcwd()+\"/languageID/\"+file_name, \"r\")\n",
    "text = data_file.read()\n",
    "\n",
    "s = text\n",
    "l = list(s)\n",
    "random.shuffle(l)\n",
    "text = ''.join(l)\n",
    "data_file.close()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish\n"
     ]
    }
   ],
   "source": [
    "training_data = get_training_data()\n",
    "\n",
    "test_data = {}\n",
    "for char in s:\n",
    "    test_data[char] = text.count(char)\n",
    "    \n",
    "print(make_prediction(training_data, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
